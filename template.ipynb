{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nurVfJlvJHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1158e9fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.linspace(0, 3*np.pi, 500)\n",
    "plt.plot(x, np.sin(x**2))\n",
    "plt.title('A simple chirp');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"1\" n_features=2919 grp=0 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"1\" n_features=2919 grp=0 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"1\" n_features=2919 grp=0 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"6\" n_features=2919 grp=1 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"6\" n_features=2919 grp=1 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"6\" n_features=2919 grp=1 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"1\" n_features=2919 grp=2 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"1\" n_features=2919 grp=2 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"1\" n_features=2919 grp=2 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"1\" n_features=2919 grp=3 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"1\" n_features=2919 grp=3 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"1\" n_features=2919 grp=3 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"4\" n_features=2919 grp=4 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"4\" n_features=2919 grp=4 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"4\" n_features=2919 grp=4 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"7\" n_features=2919 grp=5 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"7\" n_features=2919 grp=5 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"7\" n_features=2919 grp=5 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"1\" n_features=2919 grp=6 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"1\" n_features=2919 grp=6 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"1\" n_features=2919 grp=6 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"2\" n_features=2919 grp=7 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"2\" n_features=2919 grp=7 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"2\" n_features=2919 grp=7 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"5\" n_features=2919 grp=8 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"5\" n_features=2919 grp=8 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"5\" n_features=2919 grp=8 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"4\" n_features=2919 grp=9 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"4\" n_features=2919 grp=9 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"4\" n_features=2919 grp=9 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"2\" n_features=2919 grp=10 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"2\" n_features=2919 grp=10 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"2\" n_features=2919 grp=10 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"2\" n_features=2919 grp=11 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"2\" n_features=2919 grp=11 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"2\" n_features=2919 grp=11 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"3\" n_features=2919 grp=12 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"3\" n_features=2919 grp=12 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"3\" n_features=2919 grp=12 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"5\" n_features=2919 grp=13 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"5\" n_features=2919 grp=13 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"5\" n_features=2919 grp=13 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"4\" n_features=2919 grp=14 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"4\" n_features=2919 grp=14 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"4\" n_features=2919 grp=14 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"2\" n_features=2919 grp=15 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"2\" n_features=2919 grp=15 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"2\" n_features=2919 grp=15 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"2\" n_features=2919 grp=16 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"2\" n_features=2919 grp=16 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"2\" n_features=2919 grp=16 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"5\" n_features=2919 grp=17 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"5\" n_features=2919 grp=17 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"5\" n_features=2919 grp=17 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"4\" n_features=2919 grp=18 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"4\" n_features=2919 grp=18 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"4\" n_features=2919 grp=18 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"2\" n_features=2919 grp=19 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"2\" n_features=2919 grp=19 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"2\" n_features=2919 grp=19 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"4\" n_features=2919 grp=20 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"4\" n_features=2919 grp=20 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"4\" n_features=2919 grp=20 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"6\" n_features=2919 grp=21 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"6\" n_features=2919 grp=21 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"6\" n_features=2919 grp=21 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"1\" n_features=2919 grp=22 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"1\" n_features=2919 grp=22 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"1\" n_features=2919 grp=22 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"2\" n_features=2919 grp=23 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"2\" n_features=2919 grp=23 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"2\" n_features=2919 grp=23 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"3\" n_features=2919 grp=24 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"3\" n_features=2919 grp=24 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"3\" n_features=2919 grp=24 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"5\" n_features=2919 grp=25 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"5\" n_features=2919 grp=25 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"5\" n_features=2919 grp=25 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"3\" n_features=2919 grp=26 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"3\" n_features=2919 grp=26 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"3\" n_features=2919 grp=26 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"2\" n_features=2919 grp=27 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"2\" n_features=2919 grp=27 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"2\" n_features=2919 grp=27 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"6\" n_features=2919 grp=28 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"6\" n_features=2919 grp=28 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"6\" n_features=2919 grp=28 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"3\" n_features=2919 grp=29 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"3\" n_features=2919 grp=29 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"3\" n_features=2919 grp=29 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"3\" n_features=2919 grp=30 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"3\" n_features=2919 grp=30 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"3\" n_features=2919 grp=30 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"8\" n_features=2919 grp=31 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"8\" n_features=2919 grp=31 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"8\" n_features=2919 grp=31 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"7\" n_features=2919 grp=32 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"7\" n_features=2919 grp=32 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"7\" n_features=2919 grp=32 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"6\" n_features=2919 grp=33 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"6\" n_features=2919 grp=33 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"6\" n_features=2919 grp=33 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"3\" n_features=2919 grp=34 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"3\" n_features=2919 grp=34 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"3\" n_features=2919 grp=34 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"3\" n_features=2919 grp=35 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"3\" n_features=2919 grp=35 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"3\" n_features=2919 grp=35 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"2\" n_features=2919 grp=36 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"2\" n_features=2919 grp=36 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"2\" n_features=2919 grp=36 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"5\" n_features=2919 grp=37 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"5\" n_features=2919 grp=37 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"5\" n_features=2919 grp=37 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"1\" n_features=2919 grp=38 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"1\" n_features=2919 grp=38 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"1\" n_features=2919 grp=38 seq=0 fs_col=2>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice7-l.sig\" label=\"2\" n_features=2919 grp=39 seq=0 fs_col=0>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice6-l.sig\" label=\"2\" n_features=2919 grp=39 seq=0 fs_col=1>\n",
      "LOADED  <FeatureVector \"T2_map_Resliced_Slice8-l.sig\" label=\"2\" n_features=2919 grp=39 seq=0 fs_col=2>\n",
      "NEW FEATURE SPACE FROM FILE LIST: <FeatureSpace \"medial_7_6_8_delta_bin\" n_features=8757 n_total_samples=40 n_samples_per_group=1 n_classes=8 samples_per_class=(\"1\": 6, \"6\": 4, \"4\": 5, \"7\": 2, \"2\": 10, \"5\": 5, \"3\": 7, \"8\": 1)>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from wndcharm.FeatureSpace import FeatureSpace\n",
    "f=FeatureSpace.NewFromFileOfFiles( os.path.expanduser('data/medial_7_6_8_delta_bin.fof'), long=True )\n",
    "f.ToFitFile('data/medial_7_6_8_delta_bin.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.47100000e+03   9.96700000e-03   9.09090000e-02 ...,  -5.68879000e+04\n",
      "    1.24726000e+05   7.11482000e-01]\n",
      " [  2.46700000e+03   8.28920000e-02   4.14750000e-02 ...,  -5.34658000e+04\n",
      "    1.29016000e+05   7.17171000e-01]\n",
      " [  2.30200000e+03   8.79350000e-02   1.46444000e-01 ...,  -6.22379000e+04\n",
      "    1.28003000e+05   7.28210000e-01]\n",
      " ..., \n",
      " [  2.40200000e+03   7.63360000e-02   1.62866000e-01 ...,  -6.03631000e+04\n",
      "    1.30804000e+05   7.21693000e-01]\n",
      " [  2.45000000e+03   1.28302000e-01   7.62710000e-02 ...,  -5.37427000e+04\n",
      "    1.35765000e+05   7.17335000e-01]\n",
      " [  2.43000000e+03   1.49250000e-02   4.86660000e-02 ...,  -4.15241000e+04\n",
      "    1.06458000e+05   7.17794000e-01]]\n",
      "[array([[  2.47100000e+03,   9.96700000e-03,   9.09090000e-02, ...,\n",
      "         -5.68879000e+04,   1.24726000e+05,   7.11482000e-01],\n",
      "       [  2.46700000e+03,   8.28920000e-02,   4.14750000e-02, ...,\n",
      "         -5.34658000e+04,   1.29016000e+05,   7.17171000e-01],\n",
      "       [  2.30200000e+03,   8.79350000e-02,   1.46444000e-01, ...,\n",
      "         -6.22379000e+04,   1.28003000e+05,   7.28210000e-01],\n",
      "       [  2.46200000e+03,   1.47950000e-01,   2.39520000e-02, ...,\n",
      "         -6.20287000e+04,   1.32556000e+05,   7.22202000e-01],\n",
      "       [  2.43800000e+03,   3.95870000e-02,   9.68800000e-02, ...,\n",
      "         -5.87392000e+04,   1.31402000e+05,   7.15094000e-01],\n",
      "       [  2.46100000e+03,   5.10400000e-02,   5.51820000e-02, ...,\n",
      "         -5.76776000e+04,   1.33002000e+05,   7.27425000e-01]]), array([[  2.09700000e+03,   6.25000000e-02,   8.09060000e-02, ...,\n",
      "         -6.32063000e+04,   1.25624000e+05,   7.26984000e-01],\n",
      "       [  2.45700000e+03,   1.01549000e-01,   9.32480000e-02, ...,\n",
      "         -6.33270000e+04,   1.33923000e+05,   7.24510000e-01],\n",
      "       [  2.42400000e+03,   4.84430000e-02,   7.98120000e-02, ...,\n",
      "         -5.69106000e+04,   1.27462000e+05,   7.18495000e-01],\n",
      "       [  2.45700000e+03,   1.07910000e-02,   5.99680000e-02, ...,\n",
      "         -5.72521000e+04,   1.25122000e+05,   7.15362000e-01]]), array([[  2.46800000e+03,   4.31780000e-02,   6.20270000e-02, ...,\n",
      "         -5.80721000e+04,   1.22935000e+05,   7.16681000e-01],\n",
      "       [  2.41100000e+03,   1.41880000e-01,   6.84900000e-03, ...,\n",
      "         -5.69183000e+04,   1.37633000e+05,   7.31685000e-01],\n",
      "       [  2.42300000e+03,   8.15600000e-02,   1.52910000e-02, ...,\n",
      "         -5.82305000e+04,   1.41897000e+05,   7.19255000e-01],\n",
      "       [  2.44800000e+03,   2.89860000e-02,   9.12050000e-02, ...,\n",
      "         -6.30544000e+04,   1.28148000e+05,   7.22534000e-01],\n",
      "       [  2.45500000e+03,   1.82100000e-03,   9.34150000e-02, ...,\n",
      "         -6.07223000e+04,   1.27807000e+05,   7.18808000e-01]]), array([[  2.36400000e+03,   2.29890000e-02,   1.65109000e-01, ...,\n",
      "         -6.24460000e+04,   1.27172000e+05,   7.16624000e-01],\n",
      "       [  2.42000000e+03,   3.01720000e-02,   2.55878000e-01, ...,\n",
      "         -6.30226000e+04,   1.41483000e+05,   7.17040000e-01]]), array([[  2.43600000e+03,   1.70213000e-01,   1.56250000e-01, ...,\n",
      "         -5.18181000e+04,   1.41752000e+05,   7.12934000e-01],\n",
      "       [  2.40200000e+03,   8.89680000e-02,   9.52400000e-03, ...,\n",
      "         -5.69199000e+04,   1.25305000e+05,   7.08236000e-01],\n",
      "       [  2.38300000e+03,   8.58140000e-02,   1.34230000e-02, ...,\n",
      "         -5.68559000e+04,   1.35509000e+05,   7.16314000e-01],\n",
      "       ..., \n",
      "       [  2.44100000e+03,   1.23389000e-01,   1.98083000e-01, ...,\n",
      "         -6.18272000e+04,   1.28877000e+05,   7.33498000e-01],\n",
      "       [  2.39400000e+03,   7.52690000e-02,   1.33676000e-01, ...,\n",
      "         -6.33295000e+04,   1.29084000e+05,   7.18021000e-01],\n",
      "       [  2.45300000e+03,   9.92910000e-02,   1.74089000e-01, ...,\n",
      "         -6.37833000e+04,   1.18731000e+05,   7.15956000e-01]]), array([[  2.47000000e+03,   4.04410000e-02,   1.94904000e-01, ...,\n",
      "         -6.39740000e+04,   1.28504000e+05,   7.26436000e-01],\n",
      "       [  2.42200000e+03,   5.28110000e-02,   6.77970000e-02, ...,\n",
      "         -5.86787000e+04,   1.32068000e+05,   7.29644000e-01],\n",
      "       [  2.17300000e+03,   1.36719000e-01,   1.68450000e-02, ...,\n",
      "         -6.16052000e+04,   1.26405000e+05,   7.24288000e-01],\n",
      "       [  2.43200000e+03,   1.44312000e-01,   7.37460000e-02, ...,\n",
      "         -5.17925000e+04,   1.17332000e+05,   7.18500000e-01],\n",
      "       [  2.43400000e+03,   1.76015000e-01,   3.07801000e-01, ...,\n",
      "         -5.69183000e+04,   1.38707000e+05,   7.24818000e-01]]), array([[  2.32500000e+03,   6.27310000e-02,   1.06220000e-02, ...,\n",
      "         -6.34618000e+04,   1.22814000e+05,   7.18513000e-01],\n",
      "       [  2.23100000e+03,   1.09375000e-01,   1.45427000e-01, ...,\n",
      "         -6.16136000e+04,   1.27906000e+05,   7.27635000e-01],\n",
      "       [  2.46600000e+03,   3.67600000e-03,   5.42640000e-02, ...,\n",
      "         -2.91663000e+04,   1.04807000e+05,   7.11990000e-01],\n",
      "       ..., \n",
      "       [  2.46800000e+03,   5.16610000e-02,   1.54971000e-01, ...,\n",
      "         -5.60158000e+04,   1.16523000e+05,   7.13656000e-01],\n",
      "       [  2.40200000e+03,   7.63360000e-02,   1.62866000e-01, ...,\n",
      "         -6.03631000e+04,   1.30804000e+05,   7.21693000e-01],\n",
      "       [  2.45000000e+03,   1.28302000e-01,   7.62710000e-02, ...,\n",
      "         -5.37427000e+04,   1.35765000e+05,   7.17335000e-01]]), array([[  2.43000000e+03,   1.49250000e-02,   4.86660000e-02, ...,\n",
      "         -4.15241000e+04,   1.06458000e+05,   7.17794000e-01]])]\n",
      "['1', '6', '1', '1', '4', '7', '1', '2', '5', '4', '2', '2', '3', '5', '4', '2', '2', '5', '4', '2', '4', '6', '1', '2', '3', '5', '3', '2', '6', '3', '3', '8', '7', '6', '3', '3', '2', '5', '1', '2']\n",
      "[1.0, 6.0, 1.0, 1.0, 4.0, 7.0, 1.0, 2.0, 5.0, 4.0, 2.0, 2.0, 3.0, 5.0, 4.0, 2.0, 2.0, 5.0, 4.0, 2.0, 4.0, 6.0, 1.0, 2.0, 3.0, 5.0, 3.0, 2.0, 6.0, 3.0, 3.0, 8.0, 7.0, 6.0, 3.0, 3.0, 2.0, 5.0, 1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "dmat = f.ContiguousDataMatrix()\n",
    "print (dmat)\n",
    "print (f.data_list)\n",
    "print (f._contiguous_ground_truth_labels)\n",
    "print (f._contiguous_ground_truth_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '6', '4', '7', '2', '5', '3', '8']\n",
      "[1.0, 6.0, 4.0, 7.0, 2.0, 5.0, 3.0, 8.0]\n"
     ]
    }
   ],
   "source": [
    "print (f.class_names)\n",
    "class_vals = [float(x) for x in f.class_names]\n",
    "print (class_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.lda import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize (train, test):\n",
    "    norm_train_set = train.copy() \n",
    "    mins, maxs = normalize_by_columns (norm_train_set)\n",
    "    norm_test_set = test.copy() \n",
    "    normalize_by_columns (norm_test_set, mins, maxs)\n",
    "    return (norm_train_set, norm_test_set)\n",
    "\n",
    "def normalize_by_columns ( full_stack, mins = None, maxs = None ):\n",
    "    \"\"\"This is a global function to normalize a matrix by columns.\n",
    "    If numpy 1D arrays of mins and maxs are provided, the matrix will be normalized against these ranges\n",
    "    Otherwise, the mins and maxs will be determined from the matrix, and the matrix will be normalized\n",
    "    against itself. The mins and maxs will be returned as a tuple.\n",
    "    Out of range matrix values will be clipped to min and max (including +/- INF)\n",
    "    zero-range columns will be set to 0.\n",
    "    NANs in the columns will be set to 0.\n",
    "    The normalized output range is hard-coded to 0-100\n",
    "    \"\"\"\n",
    "    # Edge cases to deal with:\n",
    "    # Range determination:\n",
    "    # 1. features that are nan, inf, -inf\n",
    "    # max and min determination must ignore invalid numbers\n",
    "    # nan -> 0, inf -> max, -inf -> min\n",
    "    # Normalization:\n",
    "    # 2. feature values outside of range\n",
    "    # values clipped to range (-inf to min -> min, max to inf -> max) - leaves nan as nan\n",
    "    # 3. feature ranges that are 0 result in nan feature values\n",
    "    # 4. all nan feature values set to 0\n",
    "\n",
    "    # Turn off numpy warnings, since we're taking care of invalid values explicitly\n",
    "    oldsettings = np.seterr(all='ignore')\n",
    "    if (mins is None or maxs is None):\n",
    "        # mask out NANs and +/-INFs to compute min/max\n",
    "        full_stack_m = np.ma.masked_invalid (full_stack, copy=False)\n",
    "        maxs = full_stack_m.max (axis=0)\n",
    "        mins = full_stack_m.min (axis=0)\n",
    "\n",
    "    # clip the values to the min-max range (NANs are left, but +/- INFs are taken care of)\n",
    "    full_stack.clip (mins, maxs, full_stack)\n",
    "    # remake a mask to account for NANs and divide-by-zero from max == min\n",
    "    full_stack_m = np.ma.masked_invalid (full_stack, copy=False)\n",
    "\n",
    "    # Normalize\n",
    "    full_stack_m -= mins\n",
    "    full_stack_m /= (maxs - mins)\n",
    "    # Left over NANs and divide-by-zero from max == min become 0\n",
    "    # Note the deep copy to change the numpy parameter in-place.\n",
    "    full_stack[:] = full_stack_m.filled (0) * 100.0\n",
    "\n",
    "    # return settings to original\n",
    "    np.seterr(**oldsettings)\n",
    "\n",
    "    return (mins,maxs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stand (train, test):\n",
    "    scaler = StandardScaler()\n",
    "    new_train_set = scaler.fit_transform(train)\n",
    "    new_test_set = scaler.transform(test)\n",
    "    return (new_train_set,new_test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def round_robin_iteration (index, data_matrix_list):\n",
    "    '''Does a leave N out, where N is the number of classes.\n",
    "    The class with the smallest number of samples -1 (nsamples - 1) determines training set size.\n",
    "    Picks nsamples-1 for training and testing from a circular list starting at index.\n",
    "    Index ranges from 0 to the product of number of samples in each class.\n",
    "    data_matrix_list is a list of data matrixes, with one matrix per class'''\n",
    "    lengths = [m.shape[0] for m in data_matrix_list]\n",
    "    nclasses = len(lengths)\n",
    "    max_samples = min (lengths) - 1\n",
    "    indexes = [0] * nclasses\n",
    "    cl_index = index\n",
    "    for i in range (nclasses-1,0,-1):\n",
    "        indexes[i] = cl_index / lengths[i]\n",
    "        cl_index -= (indexes[i] * lengths[i])\n",
    "    indexes[0] = cl_index\n",
    "    indexes = list(reversed(indexes))\n",
    "    \n",
    "    train_mats = []\n",
    "    test_mats = []\n",
    "    for class_num in range(nclasses):\n",
    "        class_indexes = [ (count+indexes[class_num]+1) % lengths[class_num] for count in range (max_samples) ]\n",
    "        train_mats.append (np.take (data_matrix_list[class_num], class_indexes, axis=0) )\n",
    "        test_mats.append (np.take (data_matrix_list[class_num], [indexes[class_num]], axis=0) )\n",
    "    return (train_mats, test_mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 0 0]\n",
      "  [1 1 1 1 1]\n",
      "  [2 2 2 2 2]\n",
      "  [3 3 3 3 3]\n",
      "  [4 4 4 4 4]\n",
      "  [5 5 5 5 5]\n",
      "  [6 6 6 6 6]\n",
      "  [7 7 7 7 7]]\n",
      "\n",
      " [[0 1 2 3 4]\n",
      "  [0 1 2 3 4]\n",
      "  [0 1 2 3 4]\n",
      "  [0 1 2 3 4]\n",
      "  [0 1 2 3 4]\n",
      "  [0 1 2 3 4]\n",
      "  [0 1 2 3 4]\n",
      "  [0 1 2 3 4]]]\n",
      "-------\n",
      "[array([[2, 2, 2, 2, 2]]), array([[0, 1, 2, 3, 4]])]\n",
      "[array([[3, 3, 3, 3, 3],\n",
      "       [4, 4, 4, 4, 4],\n",
      "       [5, 5, 5, 5, 5],\n",
      "       [6, 6, 6, 6, 6],\n",
      "       [7, 7, 7, 7, 7],\n",
      "       [0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1]]), array([[0, 1, 2, 3, 4],\n",
      "       [0, 1, 2, 3, 4],\n",
      "       [0, 1, 2, 3, 4],\n",
      "       [0, 1, 2, 3, 4],\n",
      "       [0, 1, 2, 3, 4],\n",
      "       [0, 1, 2, 3, 4],\n",
      "       [0, 1, 2, 3, 4]])]\n"
     ]
    }
   ],
   "source": [
    "dat_mat = np.mgrid[0:8,0:5]\n",
    "print (dat_mat)\n",
    "(train,test) = round_robin_iteration (19,dat_mat)\n",
    "print ('-------')\n",
    "print (test)\n",
    "print (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_contig_mat (data_matrix_list, class_vals):\n",
    "    data_mat_contig = np.vstack (data_matrix_list)\n",
    "    class_vals_vec_list = []\n",
    "    for class_idx in range (len(data_matrix_list)):\n",
    "        class_vals_vec_list += [class_vals[class_idx]] * len (data_matrix_list[class_idx])\n",
    "    class_vals_contig = np.asarray(class_vals_vec_list)\n",
    "    return (data_mat_contig,class_vals_contig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Fisher(split):\n",
    "    \"\"\"Takes a FeatureSet_Discrete as input and calculates a Fisher score for\n",
    "    each feature. Returns a newly instantiated instance of FisherFeatureWeights.\n",
    "\n",
    "    For:\n",
    "    N = number of classes\n",
    "    F = number of features\n",
    "    It = total number of images in training set\n",
    "    Ic = number of images in a given class\n",
    "    \"\"\"\n",
    "\n",
    "    if split == None:\n",
    "        import inspect\n",
    "        form_str = 'You passed in a None as a training set to the function {0}.{1}'\t\n",
    "        raise ValueError( form_str.format( cls.__name__, inspect.stack()[1][3] ) )\n",
    "\n",
    "    # we deal with NANs/INFs separately, so turn off numpy warnings about invalid floats.\n",
    "    oldsettings = np.seterr(all='ignore')\n",
    "\n",
    "    def get_train_3d (self):\n",
    "        if self.train_3d is None:\n",
    "            self.train_3d = get_class_mat_list (self.train_set, self.train_classed_labels)\n",
    "        return (self.train_3d)\n",
    "\n",
    "    def get_test_3d (self):\n",
    "        if self.test_3d is None:\n",
    "            self.test_3d = get_class_mat_list (self.test_set, self.test_classed_labels)\n",
    "        return (self.test_3d)\n",
    "\n",
    "    #class_mats = split.get_train_3d()\n",
    "    class_mats = get_class_mat_list (split.train_set, split.train_classed_labels)\n",
    "    # 1D matrix 1 * F\n",
    "    population_means = np.mean( split.train_set, axis = 0 )\n",
    "    n_classes = class_mats.shape[0]\n",
    "    n_features = split.train_set.shape[1]\n",
    "\n",
    "    # 2D matrix shape N * F\n",
    "    intra_class_means = np.empty( [n_classes, n_features] )\n",
    "    # 2D matrix shape N * F\n",
    "    intra_class_variances = np.empty( [n_classes, n_features] )\n",
    "\n",
    "    class_index = 0\n",
    "    for class_feature_matrix in class_mats:\n",
    "        intra_class_means[ class_index ] = np.mean( class_feature_matrix, axis=0 )\n",
    "    # Note that by default, numpy divides by N instead of the more common N-1, hence ddof=1.\n",
    "        intra_class_variances[ class_index ] = np.var( class_feature_matrix, axis=0, ddof=1 )\n",
    "        class_index += 1\n",
    "\n",
    "    # 1D matrix 1 * F\n",
    "    # we deal with NANs/INFs separately, so turn off numpy warnings about invalid floats.\n",
    "    # for the record, in numpy:\n",
    "    # 1./0. = inf, 0./inf = 0., 1./inf = 0. inf/0. = inf, inf/inf = nan\n",
    "    # 0./0. = nan, nan/0. = nan, 0/nan = nan, nan/nan = nan, nan/inf = nan, inf/nan = nan\n",
    "    # We can't deal with NANs only, must also deal with pos/neg infs\n",
    "    # The masked array allows for dealing with \"invalid\" floats, which includes nan and +/-inf\n",
    "    denom = np.mean( intra_class_variances, axis = 0 )\n",
    "    denom[denom == 0] = np.nan\n",
    "    feature_weights_m = np.ma.masked_invalid (\n",
    "            ( np.square( population_means - intra_class_means ).sum( axis = 0 ) /\n",
    "        (n_classes - 1) ) / denom\n",
    "        )\n",
    "    # return numpy error settings to original\n",
    "    np.seterr(**oldsettings)\n",
    "\n",
    "    # the filled(0) method of the masked array sets all nan and infs to 0\n",
    "    fisher_values = feature_weights_m.filled(0).tolist()\n",
    "\n",
    "    return (fisher_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pearson(train_mat, train_vals):\n",
    "    \"\"\"Calculate regression parameters and correlation statistics that fully define\n",
    "    a continuous classifier.\n",
    "\n",
    "    At present the feature weights are proportional the Pearson correlation coefficient\n",
    "    for each given feature.\"\"\"\n",
    "\n",
    "    from scipy import stats\n",
    "\n",
    "    # Known issue: running stats.linregress() with np.seterr (all='raise') has caused\n",
    "    # arithmetic underflow (FloatingPointError: 'underflow encountered in stdtr' )\n",
    "    # I think this is something we can safely ignore in this function, and return settings\n",
    "    # back to normal at the end. -CEC\n",
    "    np.seterr (under='ignore')    \n",
    "\n",
    "    pearson_coeffs = np.zeros(train_mat.shape[1])\n",
    "\n",
    "    for feature_index in range( train_mat.shape[1] ):\n",
    "        slope, intercept, pearson_coeff, p_value, std_err = stats.linregress(\n",
    "            train_vals, train_mat[:,feature_index]\n",
    "        )\n",
    "\n",
    "        pearson_coeffs[feature_index] = pearson_coeff\n",
    "# We're just returning the pearsons^2 now...\n",
    "#    pearson_values = [val*val / r_val_squared_sum for val in pearson_coeffs ]\n",
    "#    pearson_coeffs = (pearson_coeffs * pearson_coeffs) / r_val_squared_sum\n",
    "    pearson_coeffs *= pearson_coeffs\n",
    "    \n",
    "\n",
    "    # Reset numpy\n",
    "    np.seterr (all='raise')\n",
    "\n",
    "    return pearson_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def marg_prob_to_pred_value (marg_probs, class_vals):\n",
    "    weighted = np.array(marg_probs)*np.array(class_vals)\n",
    "    return (np.sum(weighted))\n",
    "\n",
    "def WND5(contig_train_mat, contig_test_mat, contig_train_vals):\n",
    "    n_test_samples = contig_test_mat.shape[0]\n",
    "    n_train_samples = contig_train_mat.shape[0]\n",
    "    predicted_classes = np.zeros(n_test_samples)\n",
    "    predicted_values = np.zeros(n_test_samples)\n",
    "    \n",
    "    epsilon = np.finfo( np.float ).eps\n",
    "    testimg_idx = 0\n",
    "    trainimg_idx = 0\n",
    "    \n",
    "    for testimg_idx in range( n_test_samples ):\n",
    "        # initialize\n",
    "        class_dists = {}\n",
    "        class_counts = {}\n",
    "        classnames_list = []\n",
    "\n",
    "        for trainimg_idx in range( n_train_samples ):\n",
    "            train_class_label = contig_train_vals[trainimg_idx]\n",
    "            if not train_class_label in class_dists:\n",
    "                class_dists [train_class_label] = 0.0\n",
    "                class_counts[train_class_label] = 0.0\n",
    "                classnames_list.append (train_class_label)\n",
    "\n",
    "            dists = np.absolute (contig_train_mat [trainimg_idx] - contig_test_mat [testimg_idx])\n",
    "            w_dist = np.sum( dists )\n",
    "            if w_dist > epsilon:\n",
    "                class_counts[train_class_label] += 1.0\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            w_dist = np.sum( np.square( dists ) )\n",
    "            # The exponent -5 is the \"5\" in \"WND5\"\n",
    "            class_dists[ train_class_label ] += w_dist ** -5\n",
    "\n",
    "        \n",
    "        class_idx = 0\n",
    "        class_similarities = [0]*len(class_dists)\n",
    "        for class_label in classnames_list:\n",
    "            class_similarities[class_idx] = class_dists[class_label] / class_counts[class_label]\n",
    "            class_idx += 1\n",
    "\n",
    "        norm_factor = sum( class_similarities )\n",
    "        marg_probs = np.array( [ x / norm_factor for x in class_similarities ] )\n",
    "\n",
    "        predicted_class_idx = marg_probs.argmax()\n",
    "\n",
    "        predicted_classes[testimg_idx] = classnames_list[ predicted_class_idx ]\n",
    "        predicted_values[testimg_idx] = marg_prob_to_pred_value (marg_probs, classnames_list)\n",
    "\n",
    "    return (predicted_classes, predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_by_weight (the_mat, feature_weights):\n",
    "    i = np.argsort(feature_weights)\n",
    "    sort_mat = the_mat[:,i]\n",
    "    sort_mat = np.fliplr(sort_mat)\n",
    "    return (sort_mat)\n",
    "\n",
    "def weigh_sort(train, test, feature_weights):\n",
    "    weigh_train = np.multiply (norm_train, feature_weights)\n",
    "    weigh_test = np.multiply (norm_test, feature_weights)\n",
    "\n",
    "    sorted_train = sort_by_weight (weigh_train, feature_weights)\n",
    "    sorted_test = sort_by_weight (weigh_test, feature_weights)\n",
    "    return (sorted_train, sorted_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_forest_clf (contig_train_mat, contig_test_mat, contig_train_vals, rnd_state = None):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators = 30, random_state = rnd_state)\n",
    "    clf.fit(contig_train_mat, contig_train_vals)\n",
    "    predicted_classes = clf.predict(contig_test_mat)\n",
    "    return (predicted_classes)\n",
    "def rand_forest_reg (contig_train_mat, contig_test_mat, contig_train_vals, rnd_state = None):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    forest = RandomForestRegressor(n_estimators=30, random_state = rnd_state)\n",
    "    forest.fit(contig_train_mat, contig_train_vals)\n",
    "    predicted = forest.predict(contig_test_mat)\n",
    "    return (predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lin_reg(contig_train_mat, contig_test_mat, contig_train_vals):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(contig_train_mat, contig_train_vals)\n",
    "    predicted = lin_reg.predict(contig_test_mat)\n",
    "    return (predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nfeatures = 200\n",
    "niter = np.product ( [m.shape[0] for m in f.data_list] )\n",
    "class_vals = [float(x) for x in f.class_names]\n",
    "n_correct = np.asarray( [0]*2)\n",
    "(train,test) = round_robin_iteration (0,f.data_list)\n",
    "print ('Train class sizes : {}'.format([x.shape[0] for x in train]))\n",
    "print ('Features          : {}'.format(nfeatures))\n",
    "print ('Iterations        : {}'.format(niter))\n",
    "for iter_idx in range ( niter ):\n",
    "    # Split\n",
    "    (train,test) = round_robin_iteration (iter_idx,f.data_list)\n",
    "    (contig_train_mat, contig_train_vals) = list_to_contig_mat (train, class_vals)\n",
    "    (contig_test_mat, contig_test_vals) = list_to_contig_mat (test, class_vals)\n",
    "\n",
    "    # Normalize\n",
    "    (norm_train, norm_test) = normalize (contig_train_mat, contig_test_mat)\n",
    "    \n",
    "    # Reduce\n",
    "    feature_weights = Pearson(norm_train, contig_train_vals)\n",
    "    (sorted_train, sorted_test) = weigh_sort (norm_train, norm_test, feature_weights)\n",
    "\n",
    "    # Classify\n",
    "    preds,pred_vals = WND5(sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals)\n",
    "\n",
    "#    preds = rand_forest_clf (sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals, iter_idx)\n",
    "#    preds = rand_forest_clf (sorted_train[:,:nfeatures], np.asarray([norm_test[1][:,:nfeatures],sorted_test[0][:,:nfeatures]]), contig_train_vals, iter_idx)\n",
    "    for pred_idx in range (len(preds)):\n",
    "        if (preds[pred_idx] == contig_test_vals[pred_idx]):\n",
    "            n_correct[pred_idx] += 1\n",
    "    cumul_acc = [(float(x) / (float(iter_idx)+1.0)) for x in n_correct]\n",
    "    print ('Iteration {}; Predictions: {}; cumulative accuracies: {}'.format(iter_idx, preds, cumul_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    200 features, RFC\n",
    "    Iteration 1119; Predictions: [ 0.  0.]; cumulative accuracies: [0.73125, 0.7294642857142857]\n",
    "    200 features, WND5\n",
    "    Iteration 1119; Predictions: [ 0.  0.]; cumulative accuracies: [0.7366071428571429, 0.7598214285714285]\n",
    "    100 features, WND5\n",
    "    Iteration 1119; Predictions: [ 0.  0.]; cumulative accuracies: [0.75, 0.7205357142857143]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmat = f.ContiguousDataMatrix()\n",
    "print (dmat)\n",
    "print (f.data_list)\n",
    "print (f._contiguous_ground_truth_labels)\n",
    "print (f._contiguous_ground_truth_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iter_idx = 1\n",
    "\n",
    "print dmat\n",
    "dmat2 = np.delete (dmat,[iter_idx],axis=0)\n",
    "print\n",
    "print np.asarray([dmat2[iter_idx]])\n",
    "\n",
    "class_vals = f._contiguous_ground_truth_values\n",
    "print class_vals\n",
    "contig_train_vals = np.delete (class_vals, [iter_idx])\n",
    "contig_test_vals = np.asarray ([class_vals[iter_idx]])\n",
    "print contig_train_vals\n",
    "print contig_test_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class sizes : 39\n",
      "Features          : 100\n",
      "Iterations        : 40\n",
      "Iteration 0; Predictions: 2.3; actual: 1.0\n",
      "Iteration 1; Predictions: 4.33333333333; actual: 6.0\n",
      "Iteration 2; Predictions: 4.93333333333; actual: 1.0\n",
      "Iteration 3; Predictions: 4.8; actual: 1.0\n",
      "Iteration 4; Predictions: 2.76666666667; actual: 4.0\n",
      "Iteration 5; Predictions: 2.53333333333; actual: 7.0\n",
      "Iteration 6; Predictions: 3.46666666667; actual: 1.0\n",
      "Iteration 7; Predictions: 4.23333333333; actual: 2.0\n",
      "Iteration 8; Predictions: 3.6; actual: 5.0\n",
      "Iteration 9; Predictions: 4.53333333333; actual: 4.0\n",
      "Iteration 10; Predictions: 3.46666666667; actual: 2.0\n",
      "Iteration 11; Predictions: 4.93333333333; actual: 2.0\n",
      "Iteration 12; Predictions: 4.56666666667; actual: 3.0\n",
      "Iteration 13; Predictions: 3.43333333333; actual: 5.0\n",
      "Iteration 14; Predictions: 3.1; actual: 4.0\n",
      "Iteration 15; Predictions: 2.96666666667; actual: 2.0\n",
      "Iteration 16; Predictions: 4.06666666667; actual: 2.0\n",
      "Iteration 17; Predictions: 3.2; actual: 5.0\n",
      "Iteration 18; Predictions: 2.16666666667; actual: 4.0\n",
      "Iteration 19; Predictions: 3.3; actual: 2.0\n",
      "Iteration 20; Predictions: 2.93333333333; actual: 4.0\n",
      "Iteration 21; Predictions: 3.13333333333; actual: 6.0\n",
      "Iteration 22; Predictions: 2.96666666667; actual: 1.0\n",
      "Iteration 23; Predictions: 2.43333333333; actual: 2.0\n",
      "Iteration 24; Predictions: 3.66666666667; actual: 3.0\n",
      "Iteration 25; Predictions: 2.43333333333; actual: 5.0\n",
      "Iteration 26; Predictions: 2.43333333333; actual: 3.0\n",
      "Iteration 27; Predictions: 3.4; actual: 2.0\n",
      "Iteration 28; Predictions: 4.73333333333; actual: 6.0\n",
      "Iteration 29; Predictions: 3.13333333333; actual: 3.0\n",
      "Iteration 30; Predictions: 3.3; actual: 3.0\n",
      "Iteration 31; Predictions: 2.9; actual: 8.0\n",
      "Iteration 32; Predictions: 3.96666666667; actual: 7.0\n",
      "Iteration 33; Predictions: 2.1; actual: 6.0\n",
      "Iteration 34; Predictions: 2.46666666667; actual: 3.0\n",
      "Iteration 35; Predictions: 3.26666666667; actual: 3.0\n",
      "Iteration 36; Predictions: 2.7; actual: 2.0\n",
      "Iteration 37; Predictions: 3.53333333333; actual: 5.0\n",
      "Iteration 38; Predictions: 3.66666666667; actual: 1.0\n",
      "Iteration 39; Predictions: 3.13333333333; actual: 2.0\n",
      "R^2: 0.0197249101125, p-value: 0.387377038132\n"
     ]
    }
   ],
   "source": [
    "nfeatures = 100\n",
    "niter = dmat.shape[0]\n",
    "class_vals = f._contiguous_ground_truth_values\n",
    "n_correct = np.asarray( [0]*2)\n",
    "(train,test) = round_robin_iteration (0,f.data_list)\n",
    "print ('Train class sizes : {}'.format(dmat.shape[0]-1))\n",
    "print ('Features          : {}'.format(nfeatures))\n",
    "print ('Iterations        : {}'.format(niter))\n",
    "predictions = []\n",
    "actual = []\n",
    "for iter_idx in range ( niter ):\n",
    "    # Split\n",
    "    contig_train_mat = np.delete(dmat,[iter_idx],axis=0)\n",
    "    contig_test_mat = np.asarray([dmat[iter_idx]])\n",
    "\n",
    "    contig_train_vals = np.delete (class_vals, [iter_idx])\n",
    "    contig_test_vals = np.asarray ([class_vals[iter_idx]])\n",
    "\n",
    "    # Normalize\n",
    "    (norm_train, norm_test) = normalize (contig_train_mat, contig_test_mat)\n",
    "    \n",
    "    # Reduce\n",
    "    feature_weights = Pearson(norm_train, contig_train_vals)\n",
    "    (sorted_train, sorted_test) = weigh_sort (norm_train, norm_test, feature_weights)\n",
    "\n",
    "    # Classify\n",
    "    #preds,pred_val = WND5(sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals)\n",
    "    pred_val = rand_forest_reg (sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals, iter_idx)\n",
    "    #pred_val = lin_reg (sorted_train[:,:nfeatures], sorted_test[:,:nfeatures], contig_train_vals)\n",
    "    predictions.append (pred_val[0])\n",
    "    actual.append (class_vals[iter_idx])\n",
    "    print ('Iteration {}; Predictions: {}; actual: {}'.format(iter_idx, predictions[-1], actual[-1]))\n",
    "\n",
    "score, p_value = pearsonr(predictions, actual)\n",
    "score *= score\n",
    "print ('R^2: {}, p-value: {}'.format (score, p_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    200 features RandForReg\n",
    "    R^2: 0.100336411037, p-value: 0.0464318824205\n",
    "    20 features lin reg:\n",
    "    R^2: 0.0491493694884, p-value: 0.169178264743\n",
    "    20 features RandForReg\n",
    "    R^2: 0.0595653398403, p-value: 0.129095037849\n",
    "    100 features RandForReg\n",
    "    R^2: 0.0395069277481, p-value: 0.218869033287\n",
    "    100 features RandForReg - binned, 8 bins.\n",
    "    R^2: 0.0197249101125, p-value: 0.387377038132\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
